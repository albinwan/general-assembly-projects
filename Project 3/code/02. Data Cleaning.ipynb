{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and concat datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gainit = pd.read_csv('../datasets/gainit_raw.csv')\n",
    "loseit = pd.read_csv('../datasets/loseit_raw.csv')\n",
    "gainit2 = pd.read_csv('../datasets/gainit_raw2.csv')\n",
    "loseit2 = pd.read_csv('../datasets/loseit_raw2.csv')\n",
    "\n",
    "gainit = pd.concat([gainit, gainit2], ignore_index=True)\n",
    "loseit = pd.concat([loseit, loseit2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning before train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only columns that we're interested in (`subreddit`, `selftext` and `title`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gainit.drop(gainit.columns.difference(['subreddit','selftext','title']), 1, inplace=True)\n",
    "loseit.drop(loseit.columns.difference(['subreddit','selftext','title']), 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gainit.drop_duplicates(subset='selftext', inplace=True)\n",
    "loseit.drop_duplicates(subset='selftext', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataset with combined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = pd.concat([gainit, loseit], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove null values for `selftext`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full['selftext'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = full[full['selftext'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `combined` feature by combining `title` and `selftext` so that we capture both sets of data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "full['combined'] = full['title'] + \" \" + full['selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1441, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map `y` targets to `1` and `0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "full['subreddit'] = full['subreddit'].map({'loseit': 0, 'gainit': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create separate datasets for `gainit` and `loseit` for EDA purpose in the next workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gainit_eda = full[full['subreddit'] == 1]['combined']\n",
    "loseit_eda = full[full['subreddit'] == 0]['combined']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = full['combined']\n",
    "y = full['subreddit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check baseline score for model to beat (50%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.500347\n",
       "0    0.499653\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to measure the effectiveness of our classifier model, we want to beat the baseline score which is 50%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do a train-test split in order to get a test \"unseen\" data that our model can be measured on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.33,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(965,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(476,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean words in comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to take in a string and clean it. For our model I had actually done a comparison of Lemmatizer vs. Porter Stemmer and it turns out that Lemmatizer can a slight better accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comments_to_words(raw_comment):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    #Remove URL from content\n",
    "    review_text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', raw_comment, flags=re.MULTILINE)\n",
    "    \n",
    "    #Remove non-letters\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    \n",
    "    #Convert to lower case, split into individual words.\n",
    "    words = letters_only.lower().split()\n",
    "   \n",
    "    #Lemmatize words\n",
    "    lemmatized = [lemmatizer.lemmatize(i) for i in words]\n",
    "    \n",
    "    #Remove stopwords\n",
    "    stops = set(stopwords.words('english'))\n",
    "    meaningful_words = [w for w in lemmatized if not w in stops]\n",
    "    \n",
    "    return(\" \".join(meaningful_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through comments and call on cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_comments = []\n",
    "clean_test_comments = []\n",
    "\n",
    "gainit_eda_comments = []\n",
    "loseit_eda_comments = []\n",
    "\n",
    "for train_comment in X_train:\n",
    "    clean_train_comments.append(comments_to_words(train_comment))\n",
    "\n",
    "for test_comment in X_test:\n",
    "    clean_test_comments.append(comments_to_words(test_comment))\n",
    "    \n",
    "for gain_comment in gainit_eda:\n",
    "    gainit_eda_comments.append(comments_to_words(gain_comment))\n",
    "    \n",
    "for lose_comment in loseit_eda:\n",
    "    loseit_eda_comments.append(comments_to_words(lose_comment))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize both datasets for word cloud EDA later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(stop_words = text.ENGLISH_STOP_WORDS.union(['gainit', 'loseit']))\n",
    "gainit_cvec = cvec.fit_transform(gainit_eda)\n",
    "gainit_cvec_eda = pd.DataFrame(gainit_cvec.todense(), columns=cvec.get_feature_names())\n",
    "\n",
    "cvec2 = CountVectorizer(stop_words = text.ENGLISH_STOP_WORDS.union(['gainit', 'loseit']))\n",
    "loseit_cvec = cvec2.fit_transform(loseit_eda)\n",
    "loseit_cvec_eda = pd.DataFrame(loseit_cvec.todense(), columns=cvec2.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to dataframes and export datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_train_comments = pd.DataFrame(clean_train_comments, columns=['comments'])\n",
    "df_clean_test_comments = pd.DataFrame(clean_test_comments, columns=['comments'])\n",
    "\n",
    "df_clean_train_comments.to_csv('../datasets/X_train_clean.csv', index=False)\n",
    "df_clean_test_comments.to_csv('../datasets/X_test_clean.csv', index=False)\n",
    "y_train.to_csv('../datasets/y_train.csv', index=False)\n",
    "y_test.to_csv('../datasets/y_test.csv', index=False)\n",
    "full.to_csv('../datasets/full.csv', index=False)\n",
    "\n",
    "gainit_cvec_eda.to_csv('../datasets/gainit_cvec_eda.csv', index=False)\n",
    "loseit_cvec_eda.to_csv('../datasets/loseit_cvec_eda.csv', index=False)\n",
    "\n",
    "gainit_eda.to_csv('../datasets/gainit_eda.csv', index=False)\n",
    "loseit_eda.to_csv('../datasets/loseit_eda.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "269.889px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
